{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLFlx7u8fAMH"
   },
   "source": [
    "**Object & Face recognition and Tracking**\n",
    "\n",
    "**Name**: Hriddhi Doley\n",
    "**Github link**: https://github.com/HriddhiDoley/Computer-Vision-Object-Tracking\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWheRnwAgz7N"
   },
   "source": [
    "# Task 1: Collect a short video & prepare ground truth\n",
    "\n",
    "* The short video is aligned with the list of MS COCO class names.\n",
    "  * The video is of 13s length and it is in .mp4 format\n",
    "  * The video captures a moving person,a sport ball and a hat and few static items viz. cup, teddy bear and a book.\n",
    "* Ground Truth is prepared using a tool called Computer vision Annotation Tool(CVAT).\n",
    "*The Output of CVAT is in .xml format and the below code extracts the frame, object ID and the bounding boxes and export into a .csv file for future evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5qY935liPir"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as tvtf\n",
    "from google.colab.patches import cv2_imshow  # Use this for displaying images in Colab\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZRQpgj9iTDH"
   },
   "source": [
    "## Forming the COCO Dataset class label\n",
    "* Source: https://github.com/amikelive/coco-labels/blob/master/coco-labels-paper.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5-fJ3EzjL6H"
   },
   "outputs": [],
   "source": [
    "# COCO dataset class labels\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    'background',  # Added as index 0 for alignment with object detection models\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n",
    "    'truck', 'boat', 'traffic light', 'fire hydrant', 'street sign',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat',\n",
    "    'backpack', 'umbrella', 'shoe', 'eye glasses', 'handbag', 'tie',\n",
    "    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
    "    'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "    'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork',\n",
    "    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "    'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window',\n",
    "    'desk', 'toilet', 'door', 'tv', 'laptop', 'mouse', 'remote',\n",
    "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors',\n",
    "    'teddy bear', 'hair drier', 'toothbrush', 'hair brush'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZiYH61Yiv4-"
   },
   "source": [
    "## Extract bounding boxes coordinates and frames from .xml generated by CVAT\n",
    "\n",
    "* Extract frame no, object ID and bounding boxes coordinates from the .xml file and dump these in a .csv file for future evaluation.\n",
    "  *  Input is annotations.xml\n",
    "  *  Output is groundtruth.csv, This file is later divided by objects for individual evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAB3pEnsjBgJ"
   },
   "outputs": [],
   "source": [
    "# Define the function to parse XML and dump data into a CSV file\n",
    "def xml_to_csv(xml_file, csv_file):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        # Write the headers to the CSV\n",
    "        headers = ['track_id', 'label', 'source', 'frame', 'keyframe', 'outside', 'occluded', 'xtl', 'ytl', 'xbr', 'ybr', 'z_order']\n",
    "        csvwriter.writerow(headers)\n",
    "\n",
    "        # Iterate through each track in the XML file\n",
    "        for track in root.findall('track'):\n",
    "            track_id = track.get('id')\n",
    "            label = track.get('label')\n",
    "            source = track.get('source')\n",
    "\n",
    "            # Iterate through each box within the track\n",
    "            for box in track.findall('box'):\n",
    "                frame = box.get('frame')\n",
    "                keyframe = box.get('keyframe')\n",
    "                outside = box.get('outside')\n",
    "                occluded = box.get('occluded')\n",
    "                xtl = box.get('xtl')\n",
    "                ytl = box.get('ytl')\n",
    "                xbr = box.get('xbr')\n",
    "                ybr = box.get('ybr')\n",
    "                z_order = box.get('z_order')\n",
    "\n",
    "                # Write each row to the CSV file\n",
    "                csvwriter.writerow([track_id, label, source, frame, keyframe, outside, occluded, xtl, ytl, xbr, ybr, z_order])\n",
    "\n",
    "# Execution\n",
    "xml_file = '/content/annotations.xml'  # Path to the XML file\n",
    "csv_file = 'groundtruth.csv'  # Path to the output CSV file\n",
    "\n",
    "xml_to_csv(xml_file, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLi38wj1f6s3"
   },
   "source": [
    "# Task 2: Obect Tracking and Recognition\n",
    "\n",
    "* The model used for object detection is a pre-trained Mask R-CNN\n",
    "* Objects are detected frame-by-frame in the video (task1.mp4)\n",
    "* Association method IoU is used to unique object IDs and track the objects\n",
    "* Bounding Boxes and labels are drawn on the output video (task2.mp4) showing which object ID corresponds to which ID, with labels and scores.\n",
    "* Confidence threshold of 0.5 is used to filter out low confidence predictions.\n",
    "* A .csv file is also generated to capture the frame, object ID and bounding boxes coordiantes for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02OTcU8RlI72"
   },
   "source": [
    "## Association method : IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdjc3cHjgZtB"
   },
   "outputs": [],
   "source": [
    "# Function to compute IoU between two bounding boxes using numpy for optimization\n",
    "def compute_iou(boxA, boxB):\n",
    "    xA = np.maximum(boxA[0], boxB[0])\n",
    "    yA = np.maximum(boxA[1], boxB[1])\n",
    "    xB = np.minimum(boxA[2], boxB[2])\n",
    "    yB = np.minimum(boxB[3], boxB[3])\n",
    "\n",
    "    interArea = np.maximum(0, xB - xA + 1) * np.maximum(0, yB - yA + 1)\n",
    "\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHRiH1qEogGm"
   },
   "source": [
    "## Object tracking dictionary initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uxu59vb1oj-z"
   },
   "outputs": [],
   "source": [
    "object_tracker = {}\n",
    "next_object_id = 0\n",
    "iou_threshold = 0.3  # IoU threshold to associate objects between frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGx8lUy3ozRN"
   },
   "source": [
    "## CSV logging function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJQpjnN9o4Af"
   },
   "outputs": [],
   "source": [
    "def log_to_csv(frame_num, object_id, bbox, csv_writer, label, score):\n",
    "    \"\"\"Log object detection data to CSV file.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox.astype(int)\n",
    "    csv_writer.writerow([frame_num, object_id, label, score, x1, y1, x2, y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHd5gTlho3ap"
   },
   "source": [
    "## Update tracker function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3-3xEJ7pN3u"
   },
   "outputs": [],
   "source": [
    "# Function to update tracker with current frame's detections\n",
    "def update_tracker(current_boxes):\n",
    "    global next_object_id\n",
    "    current_ids = []\n",
    "    for new_box in current_boxes:\n",
    "        matched = False\n",
    "        for obj_id, prev_box in object_tracker.items():\n",
    "            iou = compute_iou(new_box, prev_box)\n",
    "            if iou > iou_threshold:\n",
    "                object_tracker[obj_id] = new_box  # Update tracked object with new box\n",
    "                current_ids.append(obj_id)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            object_tracker[next_object_id] = new_box  # Assign new ID\n",
    "            current_ids.append(next_object_id)\n",
    "            next_object_id += 1\n",
    "    return current_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZG4iTdkpvaS"
   },
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB1RgDV0pzD2"
   },
   "outputs": [],
   "source": [
    "# Preprocessing transform\n",
    "def preprocess(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = tvtf.to_tensor(frame_rgb).to(device)\n",
    "    return img_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ix-HzHi8p1tM"
   },
   "source": [
    "## Execution of the Mask R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flqJxINrp2K9"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained Mask R-CNN model from torchvision (COCO dataset)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load video\n",
    "video_input = 'task1.mp4'\n",
    "cap = cv2.VideoCapture(video_input)\n",
    "\n",
    "# Get video dimensions and set up output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('task2.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "# CSV file for logging object detection results\n",
    "csv_file = open('object_tracking_results.csv', mode='w', newline='')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "# Write the header for the CSV file\n",
    "csv_writer.writerow(['Frame', 'Object ID', 'Label', 'Score', 'x1', 'y1', 'x2', 'y2'])\n",
    "\n",
    "frame_num = 0  # Initialize frame counter\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess frame and pass through the Mask R-CNN model\n",
    "    img_tensor = preprocess(frame)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)\n",
    "\n",
    "    # Extract boxes, labels, and scores from predictions\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    current_boxes = []\n",
    "\n",
    "    # Filter out low-confidence detections\n",
    "    for i in range(len(boxes)):\n",
    "        if scores[i] > 0.5:  # Confidence threshold\n",
    "            current_boxes.append(boxes[i])\n",
    "\n",
    "    # Update tracker and get object IDs for current frame\n",
    "    current_ids = update_tracker(current_boxes)\n",
    "\n",
    "    # Loop over the boxes and assign object IDs\n",
    "    for box, obj_id, label_id, score in zip(current_boxes, current_ids, labels, scores):\n",
    "        (x1, y1, x2, y2) = box.astype(int)\n",
    "        label = COCO_INSTANCE_CATEGORY_NAMES[label_id]\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Display object ID, label, and score\n",
    "        label_text = f\"ID {obj_id}, {label}: {score:.2f}\"\n",
    "        cv2.putText(frame, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Log the detection details into CSV\n",
    "        log_to_csv(frame_num, obj_id, box, csv_writer, label, score)\n",
    "\n",
    "    # Write the frame with detections to output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame (now using cv2_imshow)\n",
    "    #cv2_imshow(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Increment frame counter\n",
    "    frame_num += 1\n",
    "\n",
    "# Release video capture, writer, and CSV file\n",
    "cap.release()\n",
    "out.release()\n",
    "csv_file.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlvblhSIqfLL"
   },
   "source": [
    "# Task 3 : Face detection and tracking\n",
    "\n",
    "The faces, eyes, and smiles of the person were detected, and the face object was tracked.\n",
    "* ***Haar Cascades***: Three different pre-trained Haar Cascade classifiers are loaded :\n",
    "  * haarcascade_frontalface_default.xml for face detection\n",
    "  * haarcascade_eye.xml for detecting eyes\n",
    "  * haarcascade_smile.xml for detecting smiles.\n",
    "* ***Face Detection***: For each frame, faces are detected using face_cascade.detectMultiScale(). This function returns the coordinates of the bounding boxes around the detected faces.\n",
    "* ***Eye and Smile Detection***: Inside each detected face, detect eyes and smiles, and draw corresponding bounding boxes inside the face region.\n",
    "* ***Face Tracking***: Detected faces are tracked across frames using IoU-based matching, associating each face with a unique ID.\n",
    "* ***Bounding Box Drawing***: Bounding boxes are drawn around detected faces, eyes, and smiles. The ID of each tracked face is displayed above the face bounding box.\n",
    "* ***CSV Logging***: The face tracking information (frame number, object ID, bounding box coordinates) is logged to a CSV file (face_detection_results.csv).\n",
    "* ***Output Video***: The video with overlaid face detection results is saved as task3.mp4.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y_W0u-js1Rg"
   },
   "source": [
    "## Object tracking dictionary initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ACy0oS4s0S0"
   },
   "outputs": [],
   "source": [
    "object_tracker = {}\n",
    "next_object_id = 0\n",
    "iou_threshold = 0.3  # IoU threshold to associate objects between frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYvoVMOXsznk"
   },
   "source": [
    "## Execution of the Haar Cascade classifier Face detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9gPWW5EtHzG"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained Haar Cascade Classifiers for face, eye, and smile detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
    "\n",
    "# CSV logging function\n",
    "def log_to_csv(frame_num, object_id, bbox, csv_writer):\n",
    "    \"\"\"Log object detection data to CSV file.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox.astype(int)\n",
    "    csv_writer.writerow([frame_num, object_id, x1, y1, x2, y2])\n",
    "\n",
    "# Load video\n",
    "video_input = 'task2.mp4'\n",
    "cap = cv2.VideoCapture(video_input)\n",
    "\n",
    "# Get video dimensions and set up output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('task3.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "# CSV file for logging object detection results\n",
    "csv_file = open('face_detection_results.csv', mode='w', newline='')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "# Write the header for the CSV file\n",
    "csv_writer.writerow(['Frame', 'Object ID', 'x1', 'y1', 'x2', 'y2'])\n",
    "\n",
    "frame_num = 0  # Initialize frame counter\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale (required for Haar classifiers)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces using the Haar cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    current_boxes = []\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        current_boxes.append([x, y, x+w, y+h])\n",
    "        # Detect eyes within the face region\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, 1.7, 22)\n",
    "\n",
    "        # Draw face bounding box\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        # Draw eye bounding boxes within the face\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw smile bounding box\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(roi_color, (sx, sy), (sx+sw, sy+sh), (0, 0, 255), 2)\n",
    "\n",
    "    # Update tracker and get object IDs for current frame\n",
    "    current_ids = update_tracker(current_boxes)\n",
    "\n",
    "    # Loop over the boxes and assign object IDs\n",
    "    for box, obj_id in zip(current_boxes, current_ids):\n",
    "        (x1, y1, x2, y2) = box\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Display object ID\n",
    "        label = f\"ID {obj_id}\"\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Log the detection details into CSV\n",
    "        log_to_csv(frame_num, obj_id, np.array([x1, y1, x2, y2]), csv_writer)\n",
    "\n",
    "    # Write the frame with detections to output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame\n",
    "    # cv2_imshow(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Increment frame counter\n",
    "    frame_num += 1\n",
    "\n",
    "# Release video capture, writer, and CSV file\n",
    "cap.release()\n",
    "out.release()\n",
    "csv_file.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5INnmmSt0u7"
   },
   "source": [
    "# Task 4: Evaluation / Analyse your object and face detection, recognition and tracking results\n",
    "\n",
    "**Evaluation method:**\n",
    "* A .csv file (object_tracking_results.csv) is also generated to capture the frame, object ID and bounding boxes coordinates for the detected objects.\n",
    "* Ground Truth is prepared using a tool called Computer vision Annotation Tool(CVAT).\n",
    "* The Output of CVAT is in .xml format and the below code extracts the frame, object ID and the bounding boxes and export into a .csv file (groundtruth.csv) for evaluation.\n",
    "* Comparison of Ground Truth and Detected object/ faces\n",
    "  * The association method of IoU is used to compute the intersection over union between two bounding boxes.\n",
    "  * For each object in the ground truth, we check if there is a corresponding detected face in the same frame with an IoU greater than or equal to the threshold (0.5 in this case).\n",
    "  * If a match is found, it's counted as a true positive.\n",
    "  * If no match is found, it’s counted as a false positive.\n",
    "  * Evaluation Metrics:\n",
    "    * True Positive (TP): Correctly detected faces/objects.\n",
    "    * False Positive (FP): Detected faces/objects that don’t correspond to any ground truth face/object.\n",
    "    * False Negative (FN): Faces/objects in the ground truth that were not detected.\n",
    "    * Precision: Proportion of detected faces/objects that are correct.\n",
    "    * Recall: Proportion of actual faces/objects that were detected.\n",
    "    * F1-Score: The harmonic mean of precision and recall, representing the balance between both.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1L9aGMDwg7s"
   },
   "outputs": [],
   "source": [
    "def evaluate_object_detection(object_detection_file, groundtruth_file):\n",
    "\n",
    "\n",
    "    # Load the files into pandas DataFrames\n",
    "    object_detection_df = pd.read_csv(object_detection_file)\n",
    "    groundtruth_df = pd.read_csv(groundtruth_file)\n",
    "\n",
    "    # Define IoU threshold\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "    # Initialize counters for evaluation metrics\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    # Loop through the ground truth data and compare with the predicted data\n",
    "    for index, gt_row in groundtruth_df.iterrows():\n",
    "        gt_frame = gt_row['Frame']\n",
    "        gt_bbox = [gt_row['x1'], gt_row['y1'], gt_row['x2'], gt_row['y2']]\n",
    "\n",
    "        # Filter the predictions for the same frame\n",
    "        predicted_boxes = object_detection_df[object_detection_df['Frame'] == gt_frame]\n",
    "\n",
    "        match_found = False\n",
    "        for _, pred_row in predicted_boxes.iterrows():\n",
    "            pred_bbox = [pred_row['x1'], pred_row['y1'], pred_row['x2'], pred_row['y2']]\n",
    "\n",
    "            # Calculate IoU between ground truth and predicted box\n",
    "            iou = compute_iou(gt_bbox, pred_bbox)\n",
    "\n",
    "            if iou >= iou_threshold:\n",
    "                true_positive += 1\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            false_negative += 1\n",
    "\n",
    "    # Calculate false positives (predicted faces not matched with any ground truth)\n",
    "    for index, pred_row in object_detection_df.iterrows():\n",
    "        pred_frame = pred_row['Frame']\n",
    "        pred_bbox = [pred_row['x1'], pred_row['y1'], pred_row['x2'], pred_row['y2']]\n",
    "\n",
    "        # Filter the ground truth boxes for the same frame\n",
    "        groundtruth_boxes = groundtruth_df[groundtruth_df['Frame'] == pred_frame]\n",
    "\n",
    "        match_found = False\n",
    "        for _, gt_row in groundtruth_boxes.iterrows():\n",
    "            gt_bbox = [gt_row['x1'], gt_row['y1'], gt_row['x2'], gt_row['y2']]\n",
    "\n",
    "            # Calculate IoU between predicted box and ground truth\n",
    "            iou = compute_iou(gt_bbox, pred_bbox)\n",
    "\n",
    "            if iou >= iou_threshold:\n",
    "                match_found = True\n",
    "                break\n",
    "\n",
    "        if not match_found:\n",
    "            false_positive += 1\n",
    "\n",
    "    # Calculate Precision, Recall, and F1-Score\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Return evaluation metrics as a dictionary\n",
    "    evaluation_metrics = {\n",
    "        \"True Positive\": true_positive,\n",
    "        \"False Positive\": false_positive,\n",
    "        \"False Negative\": false_negative,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1_score\n",
    "    }\n",
    "\n",
    "    return evaluation_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0NNt_-MuOit"
   },
   "source": [
    "## Evaluation of Face Detection and Tracking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2XYYAOhuQTV",
    "outputId": "8fbd35ee-fc0b-4240-b105-3ce4292d81a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True Positive': 139, 'False Positive': 1, 'False Negative': 131, 'Precision': 0.9928571428571429, 'Recall': 0.5148148148148148, 'F1 Score': 0.6780487804878049}\n"
     ]
    }
   ],
   "source": [
    "face_detection_file_path = '/content/face_detection_results.csv'\n",
    "groundtruth_file_path = '/content/groundtruth_face.csv'\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluation_results = evaluate_object_detection(face_detection_file_path, groundtruth_file_path)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oToLkIyuTLb"
   },
   "source": [
    "## Evaluation of detection and Tracking of the object type Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSLU2joZuYpO",
    "outputId": "40a17031-5749-4797-c766-c2fa55f3d0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True Positive': 290, 'False Positive': 131, 'False Negative': 13, 'Precision': 0.6888361045130641, 'Recall': 0.9570957095709571, 'F1 Score': 0.8011049723756907}\n"
     ]
    }
   ],
   "source": [
    "person_detection_file_path = '/content/Object_Tracking_Results_Person.csv'\n",
    "groundtruth_file_path = '/content/groundtruth_person.csv'\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluation_results = evaluate_object_detection(person_detection_file_path, groundtruth_file_path)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eKq8khuuaDJ"
   },
   "source": [
    "## Evaluation of detection and Tracking of the object type Ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJ_Doqe_udyp",
    "outputId": "8f1610a3-79eb-406b-aa8a-7845c9e9c606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True Positive': 193, 'False Positive': 0, 'False Negative': 17, 'Precision': 1.0, 'Recall': 0.919047619047619, 'F1 Score': 0.9578163771712159}\n"
     ]
    }
   ],
   "source": [
    "ball_detection_file_path = '/content/Object_Tracking_Results_Ball.csv'\n",
    "groundtruth_file_path = '/content/groundtruth_ball.csv'\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluation_results = evaluate_object_detection(ball_detection_file_path, groundtruth_file_path)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9ZWdg0ZuhEi"
   },
   "source": [
    "## Evaluation of detection and Tracking of the object type Teddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-onjdZiui_d",
    "outputId": "a6c04ab4-db1f-4418-90d1-408033aebc4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True Positive': 391, 'False Positive': 1, 'False Negative': 3, 'Precision': 0.9974489795918368, 'Recall': 0.9923857868020305, 'F1 Score': 0.994910941475827}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "teddy_detection_file_path = '/content/Object_Tracking_Results_teddy.csv'\n",
    "groundtruth_file_path = '/content/groundtruth_teddy.csv'\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluation_results = evaluate_object_detection(teddy_detection_file_path, groundtruth_file_path)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZCBzF16ulCI"
   },
   "source": [
    "## Evaluation of detection and Tracking of the object type book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1dGceTGuqrx",
    "outputId": "2f03d302-e69a-4c96-9dda-e0b7212bbb19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True Positive': 272, 'False Positive': 4, 'False Negative': 124, 'Precision': 0.9855072463768116, 'Recall': 0.6868686868686869, 'F1 Score': 0.8095238095238095}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "book_detection_file_path = '/content/Object_Tracking_Results_Book.csv'\n",
    "groundtruth_file_path = '/content/groundtruth_book.csv'\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluation_results = evaluate_object_detection(book_detection_file_path, groundtruth_file_path)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VHw7OBQusiB"
   },
   "source": [
    "## Evaluation of detection and Tracking of the object type cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22Zqseq1uuIU",
    "outputId": "cac295ed-5e5b-45a3-e855-c45fd117f118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True Positive': 394, 'False Positive': 4, 'False Negative': 1, 'Precision': 0.9899497487437185, 'Recall': 0.9974683544303797, 'F1 Score': 0.9936948297604035}\n"
     ]
    }
   ],
   "source": [
    "cup_detection_file_path = '/content/Object_Tracking_Results_Cup.csv'\n",
    "groundtruth_file_path = '/content/groundtruth_cup.csv'\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluation_results = evaluate_object_detection(cup_detection_file_path, groundtruth_file_path)\n",
    "print(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
